names(tp_2020)
names(no_rt_2020)
class(no_rt_2020$created_at)
?system.time()
# minimal covariates: numeric time + baseline for category
meta$date_num <- as.numeric(meta$created_at)
# pull pieces out of `prep`
docs  <- prep$documents
vocab <- prep$vocab
meta  <- prep$meta          # <-- make sure this exists
# sanity check
stopifnot(is.data.frame(meta))
# make covariates
if (!inherits(meta$created_at, "POSIXct")) {
meta$created_at <- lubridate::ymd_hms(meta$created_at, tz = "UTC", quiet = TRUE)
}
meta$date_num <- as.numeric(meta$created_at)
meta$tweet_category <- relevel(factor(meta$tweet_category), ref = "original")
# minimal covariates: numeric time + baseline for category
meta$date_num <- as.numeric(meta$created_at)
meta$tweet_category <- relevel(factor(meta$tweet_category), ref = "original")
# search for number of topics
kgrid <- c(15, 20, 25, 30)
sk <- searchK(
documents = docs, vocab = vocab, data = meta,
K = kgrid,
prevalence = ~ tweet_category + s(date_num),
init.type = "Spectral",
verbose = TRUE
)
glimpse(sk)
# plot results. look for high exclusivity and semantic coherence
ggplot(data = as.data.frame(sk$results),
aes(x = as.numeric(semcoh), y = as.numeric(exclus))) +
geom_text(aes(label = K),
show.legend = F,
check_overlap = F,
size = 3.6,
family = "Times New Roman") +
labs(x = 'Semantic coherence', y = 'Exclusivity') +
theme_classic() +
theme(text = element_text(size = 12, family = "Times New Roman"))
summary(sk)
sk
graph(sk)
plot(sk)
dev.off()
# plot results. look for high exclusivity and semantic coherence
ggplot(data = as.data.frame(sk$results),
aes(x = as.numeric(semcoh), y = as.numeric(exclus))) +
geom_text(aes(label = K),
show.legend = F,
check_overlap = F,
size = 3.6,
family = "Times New Roman") +
labs(x = 'Semantic coherence', y = 'Exclusivity') +
theme_classic() +
theme(text = element_text(size = 12, family = "Times New Roman"))
plot(sk)
# search for number of topics
kgrid <- c(18, 20, 22, 24, 25, 26, 28)
sk2 <- searchK(
documents = docs, vocab = vocab, data = meta,
K = kgrid,
prevalence = ~ tweet_category + s(date_num),
init.type = "Spectral",
verbose = TRUE
)
plot(sk2)
# plot results. look for high exclusivity and semantic coherence
ggplot(data = as.data.frame(sk2$results),
aes(x = as.numeric(semcoh), y = as.numeric(exclus))) +
geom_text(aes(label = K),
show.legend = F,
check_overlap = F,
size = 3.6,
family = "Times New Roman") +
labs(x = 'Semantic coherence', y = 'Exclusivity') +
theme_classic() +
theme(text = element_text(size = 12, family = "Times New Roman"))
K <- 22  # or 24
mod_2020 <- stm(
documents  = docs, vocab = vocab, data = meta,
K          = K,
prevalence = ~ tweet_category + s(date_num),
init.type  = "Spectral",
max.em.its = 75,
verbose    = TRUE
)
labelTopics(mod_2020, n = 10)       # top words per topic
# PREPROCESSING FUNCTION ----
clean_text <- function(x){
# create new classifier column
x <- x %>%
mutate(
tweet_category = case_when(
sourcetweet_type == "retweeted"  ~ "retweet",
sourcetweet_type == "quoted"     ~ "quote",
sourcetweet_type == "replied_to" ~ "reply",
!is.na(in_reply_to_user_id)      ~ "reply",
TRUE                             ~ "original"
)
)
# filter out other languages
x <- x %>% dplyr::filter(lang == "en")
# remove links and duplicates
x$text <- gsub("\\bhttp\\S*\\s*", " ", x$text)
x <- x[!duplicated(x$text), ]
# normalize text
x$text <- tolower(x$text)                                         # lowercase
x$text <- gsub("\n", " ", x$text)                                 # remove newlines
x$text <- stringr::str_replace_all(
x$text, "[\u2019\u2018\u02BC\u2032\u00B4\uFF07`´]", "'") # normalize apostrophes
x$text <- stringi::stri_replace_all_regex(x$text, "[\\p{Cf}]", "") # strip zero-width chars
# expand contractions
x$text <- replace_contraction(x$text, ignore.case = FALSE, sent.cap = FALSE)
x$text <- gsub("\\bim\\b", "i am", x$text, perl = TRUE)   # fix apostrophe-less "im"
## extract hashtags
hashtags <- stringr::str_extract_all(x$text, "#\\w+")
x$text   <- gsub("#\\w+", " ", x$text)
x$extracted_hashtags <- sapply(hashtags, function(z) paste(z, collapse = " "))
## extract tagged users
tagged <- stringr::str_extract_all(x$text, "@\\w+")
x$text <- gsub("@\\w+", " ", x$text)
x$tagged <- sapply(tagged, function(z) paste(z, collapse = " "))
# general cleaning
x$text <- gsub("[[:punct:]]", " ", x$text)  # strip other punctuation
x$text <- gsub("[[:digit:]]", " ", x$text)  # remove digits
x$text <- lemmatize_strings(x$text)         # lemmatize
## handle negations
negation <- c("not ", "never ", "no ", "nobody ", "nor ", "neither ")
negation_fixed <- c("not_", "never_", "no_", "nobody_", "nor_", "neither_")
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = negation,
replacement = negation_fixed,
vectorize_all = FALSE
)
# british/canadian -> american
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = british,
replacement = american,
vectorize_all = FALSE
)
# replace abbreviations (e.g., nd -> neurodiversity)
abbrev_patterns <- paste0("\\b", abbreviate, "\\b")  # add word boundaries
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = abbrev_patterns,
replacement = abbreviation,
vectorize_all = FALSE
)
# remove artefacts
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)   # standalone RT or "RT:"
x$text <- gsub("&(?:amp|gt|lt);", " ", x$text)                   # &amp; &gt; &lt;
x$text <- gsub(" s ",  " is ", x$text)
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)
x$text <- gsub("&gt;|&lt;", " ", x$text)
# tidy whitespace
x$text <- gsub("\\s+", " ", x$text)
x$text <- trimws(x$text)
return(x)
}
temp2 <- read.csv('data/raw_data/2020/2020-Aug.csv')
temp2 <- clean_text(temp2)
View(temp2)
# PREPROCESSING FUNCTION ----
clean_text <- function(x){
# create new classifier column
x <- x %>%
mutate(
tweet_category = case_when(
sourcetweet_type == "retweeted"  ~ "retweet",
sourcetweet_type == "quoted"     ~ "quote",
sourcetweet_type == "replied_to" ~ "reply",
!is.na(in_reply_to_user_id)      ~ "reply",
TRUE                             ~ "original"
)
)
# filter out other languages
x <- x %>% dplyr::filter(lang == "en")
# remove links and duplicates
x$text <- gsub("\\bhttp\\S*\\s*", " ", x$text)
#x <- dplyr::distinct(x, text, .keep_all = TRUE)
# normalize text
x$text <- tolower(x$text)                                         # lowercase
x$text <- gsub("\n", " ", x$text)                                 # remove newlines
x$text <- stringr::str_replace_all(
x$text, "[\u2019\u2018\u02BC\u2032\u00B4\uFF07`´]", "'") # normalize apostrophes
x$text <- stringi::stri_replace_all_regex(x$text, "[\\p{Cf}]", "") # strip zero-width chars
# expand contractions
x$text <- replace_contraction(x$text, ignore.case = FALSE, sent.cap = FALSE)
x$text <- gsub("\\bim\\b", "i am", x$text, perl = TRUE)   # fix apostrophe-less "im"
## extract hashtags
hashtags <- stringr::str_extract_all(x$text, "#\\w+")
x$text   <- gsub("#\\w+", " ", x$text)
x$extracted_hashtags <- sapply(hashtags, function(z) paste(z, collapse = " "))
## extract tagged users
tagged <- stringr::str_extract_all(x$text, "@\\w+")
x$text <- gsub("@\\w+", " ", x$text)
x$tagged <- sapply(tagged, function(z) paste(z, collapse = " "))
# general cleaning
x$text <- gsub("[[:punct:]]", " ", x$text)  # strip other punctuation
x$text <- gsub("[[:digit:]]", " ", x$text)  # remove digits
x$text <- lemmatize_strings(x$text)         # lemmatize
## handle negations
negation <- c("not ", "never ", "no ", "nobody ", "nor ", "neither ")
negation_fixed <- c("not_", "never_", "no_", "nobody_", "nor_", "neither_")
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = negation,
replacement = negation_fixed,
vectorize_all = FALSE
)
# british/canadian -> american
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = british,
replacement = american,
vectorize_all = FALSE
)
# replace abbreviations (e.g., nd -> neurodiversity)
abbrev_patterns <- paste0("\\b", abbreviate, "\\b")  # add word boundaries
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = abbrev_patterns,
replacement = abbreviation,
vectorize_all = FALSE
)
# remove artefacts
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)   # standalone RT or "RT:"
x$text <- gsub("&(?:amp|gt|lt);", " ", x$text)                   # &amp; &gt; &lt;
x$text <- gsub(" s ",  " is ", x$text)
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)
x$text <- gsub("&gt;|&lt;", " ", x$text)
# tidy whitespace
x$text <- gsub("\\s+", " ", x$text)
x$text <- trimws(x$text)
return(x)
}
temp <- read.csv('data/raw_data/2020/2020-Aug.csv')
temp2 <- clean_text(temp)
any(duplicated(temp2$text))
library(dplyr)
# summary counts
dup_summary <- temp2 %>%
count(text, name = "n") %>%                  # how many times each text appears
summarize(
total_rows   = sum(n),                     # = nrow(temp2)
unique_texts = n(),                        # distinct texts
dupe_groups  = sum(n > 1),                 # texts that appear 2+ times
dupe_rows    = total_rows - unique_texts,  # rows beyond the first per text
max_group    = max(n)                      # largest duplicate cluster size
)
dup_summary
# PREPROCESSING FUNCTION ----
clean_text <- function(x){
# create new classifier column
x <- x %>%
mutate(
tweet_category = case_when(
sourcetweet_type == "retweeted"  ~ "retweet",
sourcetweet_type == "quoted"     ~ "quote",
sourcetweet_type == "replied_to" ~ "reply",
!is.na(in_reply_to_user_id)      ~ "reply",
TRUE                             ~ "original"
)
)
# filter out other languages
x <- x %>% dplyr::filter(lang == "en")
# remove links and duplicates
x$text <- gsub("\\bhttp\\S*\\s*", " ", x$text)
#x <- dplyr::distinct(x, text, .keep_all = TRUE)
# normalize text
x$text <- tolower(x$text)                                         # lowercase
x$text <- gsub("\n", " ", x$text)                                 # remove newlines
x$text <- stringr::str_replace_all(
x$text, "[\u2019\u2018\u02BC\u2032\u00B4\uFF07`´]", "'") # normalize apostrophes
x$text <- stringi::stri_replace_all_regex(x$text, "[\\p{Cf}]", "") # strip zero-width chars
# expand contractions
x$text <- replace_contraction(x$text, ignore.case = FALSE, sent.cap = FALSE)
x$text <- gsub("\\bim\\b", "i am", x$text, perl = TRUE)   # fix apostrophe-less "im"
## extract hashtags
hashtags <- stringr::str_extract_all(x$text, "#\\w+")
x$text   <- gsub("#\\w+", " ", x$text)
x$extracted_hashtags <- sapply(hashtags, function(z) paste(z, collapse = " "))
## extract tagged users
tagged <- stringr::str_extract_all(x$text, "@\\w+")
x$text <- gsub("@\\w+", " ", x$text)
x$tagged <- sapply(tagged, function(z) paste(z, collapse = " "))
# general cleaning
x$text <- gsub("[[:punct:]]", " ", x$text)  # strip other punctuation
x$text <- gsub("[[:digit:]]", " ", x$text)  # remove digits
x$text <- lemmatize_strings(x$text)         # lemmatize
## handle negations
negation <- c("not ", "never ", "no ", "nobody ", "nor ", "neither ")
negation_fixed <- c("not_", "never_", "no_", "nobody_", "nor_", "neither_")
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = negation,
replacement = negation_fixed,
vectorize_all = FALSE
)
# british/canadian -> american
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = british,
replacement = american,
vectorize_all = FALSE
)
# replace abbreviations (e.g., nd -> neurodiversity)
abbrev_patterns <- paste0("\\b", abbreviate, "\\b")  # add word boundaries
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = abbrev_patterns,
replacement = abbreviation,
vectorize_all = FALSE
)
# remove artefacts
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)   # standalone RT or "RT:"
x$text <- gsub("&(?:amp|gt|lt);", " ", x$text)                   # &amp; &gt; &lt;
x$text <- gsub(" s ",  " is ", x$text)
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)
x$text <- gsub("&gt;|&lt;", " ", x$text)
# tidy whitespace
x$text <- gsub("\\s+", " ", x$text)
x$text <- trimws(x$text)
return(x)
}
temp <- read.csv('data/raw_data/2020/2020-Aug.csv')
temp2 <- clean_text(temp)
View(temp2)
# PREPROCESSING FUNCTION ----
clean_text <- function(x){
# create new classifier column
x <- x %>%
mutate(
tweet_category = case_when(
sourcetweet_type == "retweeted"  ~ "retweet",
sourcetweet_type == "quoted"     ~ "quote",
sourcetweet_type == "replied_to" ~ "reply",
!is.na(in_reply_to_user_id)      ~ "reply",
TRUE                             ~ "original"
)
)
# filter out other languages
x <- x %>% dplyr::filter(lang == "en")
# remove links and duplicates
x$text <- gsub("\\bhttp\\S*\\s*", " ", x$text)
#x <- dplyr::distinct(x, text, .keep_all = TRUE)
# create row that flags duplicates
x <- x %>%
dplyr::mutate(
# light normalization so trivial differences don't matter
text_norm = stringr::str_squish(tolower(text))
) %>%
dplyr::add_count(text_norm, name = "dup_n") %>%   # size of the duplicate group
dplyr::group_by(text_norm) %>%
dplyr::mutate(
dup_rank    = dplyr::row_number(),              # 1 = first occurrence
is_dup_text = dup_rank > 1                      # TRUE if copy/paste duplicate
) %>%
dplyr::ungroup()
# normalize text
x$text <- tolower(x$text)                                         # lowercase
x$text <- gsub("\n", " ", x$text)                                 # remove newlines
x$text <- stringr::str_replace_all(
x$text, "[\u2019\u2018\u02BC\u2032\u00B4\uFF07`´]", "'") # normalize apostrophes
x$text <- stringi::stri_replace_all_regex(x$text, "[\\p{Cf}]", "") # strip zero-width chars
# expand contractions
x$text <- replace_contraction(x$text, ignore.case = FALSE, sent.cap = FALSE)
x$text <- gsub("\\bim\\b", "i am", x$text, perl = TRUE)   # fix apostrophe-less "im"
## extract hashtags
hashtags <- stringr::str_extract_all(x$text, "#\\w+")
x$text   <- gsub("#\\w+", " ", x$text)
x$extracted_hashtags <- sapply(hashtags, function(z) paste(z, collapse = " "))
## extract tagged users
tagged <- stringr::str_extract_all(x$text, "@\\w+")
x$text <- gsub("@\\w+", " ", x$text)
x$tagged <- sapply(tagged, function(z) paste(z, collapse = " "))
# general cleaning
x$text <- gsub("[[:punct:]]", " ", x$text)  # strip other punctuation
x$text <- gsub("[[:digit:]]", " ", x$text)  # remove digits
x$text <- lemmatize_strings(x$text)         # lemmatize
## handle negations
negation <- c("not ", "never ", "no ", "nobody ", "nor ", "neither ")
negation_fixed <- c("not_", "never_", "no_", "nobody_", "nor_", "neither_")
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = negation,
replacement = negation_fixed,
vectorize_all = FALSE
)
# british/canadian -> american
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = british,
replacement = american,
vectorize_all = FALSE
)
# replace abbreviations (e.g., nd -> neurodiversity)
abbrev_patterns <- paste0("\\b", abbreviate, "\\b")  # add word boundaries
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = abbrev_patterns,
replacement = abbreviation,
vectorize_all = FALSE
)
# remove artefacts
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)   # standalone RT or "RT:"
x$text <- gsub("&(?:amp|gt|lt);", " ", x$text)                   # &amp; &gt; &lt;
x$text <- gsub(" s ",  " is ", x$text)
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)
x$text <- gsub("&gt;|&lt;", " ", x$text)
# tidy whitespace
x$text <- gsub("\\s+", " ", x$text)
x$text <- trimws(x$text)
return(x)
}
temp2 <- clean_text(temp)
View(temp2)
View(temp2)
# PREPROCESSING FUNCTION ----
clean_text <- function(x){
# create new classifier column
x <- x %>%
mutate(
tweet_category = case_when(
sourcetweet_type == "retweeted"  ~ "retweet",
sourcetweet_type == "quoted"     ~ "quote",
sourcetweet_type == "replied_to" ~ "reply",
!is.na(in_reply_to_user_id)      ~ "reply",
TRUE                             ~ "original"
)
)
# filter out other languages
x <- x %>% dplyr::filter(lang == "en")
# remove links and duplicates
x$text <- gsub("\\bhttp\\S*\\s*", " ", x$text)
#x <- dplyr::distinct(x, text, .keep_all = TRUE)
# create row that flags duplicates
x <- x %>%
dplyr::mutate(
# light normalization so trivial differences don't matter
text_norm = stringr::str_squish(tolower(text))
) %>%
dplyr::add_count(text_norm, name = "dup_n") %>%   # size of the duplicate group
dplyr::group_by(text_norm) %>%
dplyr::mutate(
dup_rank    = dplyr::row_number(),              # 1 = first occurrence
is_dup_text = dup_rank > 1                      # TRUE if copy/paste duplicate
) %>%
dplyr::ungroup()
# normalize text
x$text <- tolower(x$text)                                         # lowercase
x$text <- gsub("\n", " ", x$text)                                 # remove newlines
x$text <- stringr::str_replace_all(
x$text, "[\u2019\u2018\u02BC\u2032\u00B4\uFF07`´]", "'") # normalize apostrophes
x$text <- stringi::stri_replace_all_regex(x$text, "[\\p{Cf}]", "") # strip zero-width chars
# expand contractions
x$text <- replace_contraction(x$text, ignore.case = FALSE, sent.cap = FALSE)
x$text <- gsub("\\bim\\b", "i am", x$text, perl = TRUE)   # fix apostrophe-less "im"
## extract hashtags
hashtags <- stringr::str_extract_all(x$text, "#\\w+")
x$text   <- gsub("#\\w+", " ", x$text)
x$extracted_hashtags <- sapply(hashtags, function(z) paste(z, collapse = " "))
## extract tagged users
tagged <- stringr::str_extract_all(x$text, "@\\w+")
x$text <- gsub("@\\w+", " ", x$text)
x$tagged <- sapply(tagged, function(z) paste(z, collapse = " "))
# general cleaning
x$text <- gsub("[[:punct:]]", " ", x$text)  # strip other punctuation
x$text <- gsub("[[:digit:]]", " ", x$text)  # remove digits
x$text <- lemmatize_strings(x$text)         # lemmatize
## handle negations
negation <- c("not ", "never ", "no ", "nobody ", "nor ", "neither ")
negation_fixed <- c("not_", "never_", "no_", "nobody_", "nor_", "neither_")
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = negation,
replacement = negation_fixed,
vectorize_all = FALSE
)
# british/canadian -> american
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = british,
replacement = american,
vectorize_all = FALSE
)
# replace abbreviations (e.g., nd -> neurodiversity)
abbrev_patterns <- paste0("\\b", abbreviate, "\\b")  # add word boundaries
x$text <- stringi::stri_replace_all_regex(
x$text,
pattern = abbrev_patterns,
replacement = abbreviation,
vectorize_all = FALSE
)
# remove artefacts
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)   # standalone RT or "RT:"
x$text <- gsub("&(?:amp|gt|lt);", " ", x$text)                   # &amp; &gt; &lt;
x$text <- gsub(" s ",  " is ", x$text)
x$text <- gsub("(?i)\\bRT\\b:?\\s*", " ", x$text, perl = TRUE)
x$text <- gsub("&gt;|&lt;", " ", x$text)
# tidy whitespace
x$text <- gsub("\\s+", " ", x$text)
x$text <- trimws(x$text)
return(x)
}
head(temp2$text, 500)
head(temp2$text, 100)
